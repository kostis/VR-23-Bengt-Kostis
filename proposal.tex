% -*- mode: LaTeX; fill-column: 78; -*-
\documentclass[11pt]{article}
\usepackage{url}
\usepackage{paralist}
\usepackage{fullpage}
\usepackage{microtype}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[
  bookmarks=true,bookmarksopen=true,bookmarksnumbered=false,%
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue
]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{enumitem}
\setlist{leftmargin=4.5mm}

\usepackage[leftmargin=1.4em]{quoting}

\renewcommand{\rmdefault}{ptm}
\setlength{\topmargin}{-30pt}
\setlength{\textheight}{1.08\textheight}

\newcommand{\mycomment}[1]{}
\newcommand{\FIX}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\UNREVISED}[1]{\textcolor{DarkGrey}{#1}}
\newcommand{\system}[1]{\mbox{\textsf{#1}}}
\newcommand{\bjcom}[1]{\textcolor{green}{\textbf{#1}}}

\newcounter{Task}
\newcommand{\task}[1]{\addtocounter{Task}{1}\paragraph{Task \theTask: #1}}
\newcommand{\significance}[1]{\vspace*{-0.5em}%
  \begin{quoting}\noindent\textbf{Significance:} #1\end{quoting}}

\newcommand{\myparagraph}{}
\let\myparagraph=\paragraph
\renewcommand{\paragraph}{\vspace{-3mm}\myparagraph}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}\small
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{.9pt plus 0.3ex}
}

\newcommand{\eg}{e.\/g.,\ }
\newcommand{\ie}{i.\/e.,\ }
\newcommand{\etal}{\textit{et al.}}


\begin{document}

\title{\bf Effective Security Testing of Network Protocol Implementations}
\author{}
\date{}

\maketitle
\thispagestyle{plain}

%\begin{abstract}
%\end{abstract}

\vspace*{-1.75em}

\section{Purpose and Aims} \label{sec:aims}
%==========================================
It made the news again! On April 14th, 2023, one of the days that this
proposal was written, the world news reported that ``\textit{Rheinmetall
  AG, a German automotive and arms manufacturer, has suffered a
  cyberattack.}'' Due to the nature of the company, which among other products
manufactures the Leopard 2 tanks, not many details were disclosed. In
some sense, we would be OK if cyberattacks that made the news were confined to
weapons and arms companies. Unfortunately, they also affect many other parts
of our society. For example, in summer~2021, one of the major supermarket
chains in Sweden was forced to an urgent, nationwide lockdown of its stores
after being attacked by the (then largest) ransomware attack of all time. Even
worse, in Sept~2020, for the first time ever, a patient’s death has been
linked to a cyberattack after ransomware disrupted emergency care at
Düsseldorf University Hospital in Germany.
%
The recent past and the current predictions paint a rather bleak picture.
According to Security Magazine (Dec 2022), ``\textit{there are over $2\,200$
attacks each day which breaks down to nearly one cyberattack every $39$ seconds.}''
%
According to a report by Cybersecurity Ventures, ``\textit{the total cost of
  cybercrime worldwide is projected to break \$8 trillion in 2023.
  In 2022, there were a record number of data breaches, with the
  global \emph{average} cost reaching \$4.35 million.}''
%
The only good news is that there also exist some reasons to be optimistic:
although the number of known 0-day exploits is breaking the record year after
year, this increase is also partly due to us now possessing better knowledge
about them and techniques to detect (and defend against) such
attacks~\cite{ONeil@MIT-TR-21}.

Although cyberattacks have many culprits, many of them are enabled by bugs and
security vulnerabilities in implementations of network protocols, often due to
nonconformance to their protocol's specification. Even seemingly innocent
deviations from the specification may expose implementations to security
risks. A major complication is that network protocols that establish secure
connections (\eg SSH, TLS, DTLS, QUIC, etc.) are \emph{complex} and
\emph{stateful}. Their implementations must carefully manage the type and
order of exchanged messages and cryptographic material, by maintaining a
(quite complicated) state machine which keeps track of how far the protocol
interaction has progressed. Any deviation from the order prescribed in the
protocol's specification may constitute anything between an inconsequential
error to a serious vulnerability. Corresponding implementation flaws, called
\emph{state machine bugs}, may then be exploitable, \eg to bypass
authentication steps or establish insecure
connections~\cite{MessyTLS@CACM-17,ruiter2015,somorovsky2016,DTLS@USENIX-20}.
Other kinds of vulnerabilities in protocol implementations are caused by
software errors such as buffer overflows, out-of-bounds reads, rounding
errors, undefined behaviour, etc. Unfortunately, the stateful nature of
protocol implementations makes it very challenging to apply commonly employed
software testing techniques such as \emph{coverage-driven fuzzing}.

The \textbf{purpose} of this proposal is to extend the power of techniques for
security testing of network protocol implementations, demonstrate their
potential on implementations of protocols which are widely used to secure the
Internet and IoT systems, and make these implementations more robust and more
secure in the process.
%
Our \textbf{main aim} with this project is to develop protocol testing
techniques and tools that are \emph{fully automated}, \emph{scalable}, and
\emph{similarly effective} as their black-box and grey-box counterparts,
which are currently used to find security vulnerabilities and bugs in stateless
programs.

To achieve the above, we intend to build upon recent techniques, a tool, and
experiences of our group (cf.\ \cref{sec:prelim}), and extend all these along
the following three dimensions, which we briefly overview below:
\begin{description}
\item[Algorithmic Improvements to Model Learning] A core ingredient for all
  security testing techniques we intend to develop or extend is to discover,
  in a fully automated way, as much information about the state machines of
\bjcom{I suggest ``stateful behavior''}
  protocol implementations as possible. The technique we will employ for this
  is \emph{model learning}~\cite{Angluin1987,Vaandrager@CACM-17}, and the
  challenge is to develop \emph{scalable} algorithms that learn models which
  are quite rich in information (\eg models whose symbols also contain
  parameters or capture data flow). Currently, no such algorithms exist;
\bjcom{I suggest: ``automated such algorithms are naive, and do not scale to actual protocol implementations}
  coming up with such model learning algorithms is not only interesting for
  security, but is also applicable to many other areas.
\item[Effective Stateful Fuzzing Techniques] We will employ the richer
  learned models for extending the power of \emph{protocol state fuzzing} and
  \emph{stateful coverage-driven fuzzing} techniques, thereby allowing them
  to automatically detect more vulnerabilities and ``deeper'' errors. Note that
  these two techniques, which we will describe in more detail in the following
  sections, are techniques which are different and complementary to each
  other. The former aims to detect protocol state machine bugs (\ie logical
  errors that result in vulnerabilities, interoperability issues, and
  specification non-conformance), while the latter typically detects software
  errors (illegal memory accesses, rounding errors, etc.).
\item[Tools, Case Studies, and Methodologies] We will implement our fuzzing
  techniques in suitable software tools, applying them to implementations of
  existing and new network protocols, which we will use as case studies. One
  purpose for running such case studies is to evaluate the effectiveness of
  our techniques, and robustify the implementations we will target. Another is
  to help us develop methodologies for the most effective uses of these
  techniques by protocol implementors, which we might also engage in user
  studies.
\end{description}


\section{State of the Art} \label{sec:SOTA}
%==========================================
Testing is and will continue to be a dominant technique for checking
correctness properties of programs and protocol implementations.  At a very
coarse level, testing techniques can be classified as \emph{black-box},
\ie those that are \emph{not} allowed to see the internals of the System Under
Test (SUT), and \emph{white-box} ones, \ie those that need to have access to
the code of the SUT (and therefore require its availability).  In the context
of \emph{fuzz testing}, the term \emph{grey-box} is often used to denote a
variant of white-box testing that can obtain only partial information from the
code of the SUT, \eg the code coverage that has been achieved.
%
This proposal is mainly concerned with black-box and grey-box protocol testing
techniques, so below we focus on them.

%\paragraph{Protocol Testing}
Many network protocols and their implementations have been thoroughly analyzed
for different kinds of vulnerabilities and bugs. In the case of the Transport
Layer Security (TLS) protocol alone, the most widely used protocol securing
the Internet, previously discovered security vulnerabilities include
cryptographic attacks (\eg Bleichenbacher's attack~\cite{Bleichenbacher1998},
CBC padding oracle attacks~\cite{Vaudenay:CBCWeaknesses:02},
Lucky13~\cite{Lucky13@SP-13}, etc.), and the famous
Heartbleed~\cite{heartbleed} bug in OpenSSL caused by a buffer over-read.
%
Effective frameworks for testing TLS implementations include
\system{FlexTLS}~\cite{FlexTLS@WOOT-15} and
\system{TLS-Attacker}~\cite{somorovsky2016}, which have been key in discovering
numerous previously unknown vulnerabilities and bugs~\cite{MessyTLS@CACM-17}.

\paragraph{Model Learning}
Model learning~\cite{Angluin1987,Vaandrager@CACM-17} (also known as
\emph{active automata learning}) is an automated black-box technique which
constructs state machine models of software and hardware components from
information obtained by providing test inputs and observing the resulting
outputs. The construction of models is critically guided by the Myhill-Nerode theorem, a central result
in automata theory. 
Model learning has been successfully used in several different
application domains, including learning interfaces of classes in software
libraries~\cite{ACMN:interfaces}, and finding bugs and vulnerabilities in
implementations of security-critical protocols
(\eg~\cite{AJUV15,ruiter2015,FJV2016,SSH@SPIN-17,DTLS@USENIX-20}).

%% When applied to network protocols,
%% model learning algorithms operate in two alternating phases: hypothesis construction and hypothesis validation.
%% During hypothesis construction, selected sequences of input symbols are sent to the SUT, observing which
%% sequences of output symbols are generated in response.
%% When certain convergence criteria are met, the 
%% learning algorithm constructs a \emph{hypothesis}, which is a minimal state machine that is consistent with
%% the observations recorded so far, and
%% moves to the validation phase, in which the SUT is subject to a conformance testing algorithm which aims to validate that
%% the behavior of the SUT agrees with the hypothesis.
%% %% thoroughly test for input sequences for which the hypothesis mispredicts the response by the SUT.
%% If conformance testing finds a counterexample, i.e., an input sequence on which the SUT and the hypothesis disagree, the
%% hypothesis construction phase is reentered in order
%% to build a more refined hypothesis which also takes the discovered
%% counterexample into account. If no counterexample is found, learning terminates and returns the current hypothesis.

Model learning algorithms work in practice with finite input alphabets of modest sizes. In order to learn realistic SUTs, for which the alphabet of packets can be huge, the learning setup is typically extended with
a so-called \emph{mapper}, which
% acts as a test harness that
transforms input symbols from the finite alphabet known to the learning algorithm to actual protocol
messages sent to the SUT~\cite{AJUV15}.
Typically, the input alphabet consists of different types of messages, often
refined to represent interesting variations, e.g., concerning the key exchange algorithm.
%% The mapper transforms each such message to a SUT message by
%% supplying message parameters, performing cryptographic operations, etc.
The mapper also maintains state that is hidden from the
learning algorithm but needed for supplying message parameters; this can include sequence numbers, agreed encryption keys, etc.

The most widely known algorithm for model learning of finite automata 
is Angluin's $L^*$~\cite{Angluin1987}.
% , which has, e.g., been implemented in the LearnLib framework~\cite{IHS2015}.
A key strengh of model learning is that, by exploiting the Myhill-Nerode theorem, it produce succinct models of
the externally observable behavior of the SUT.
%
This allows it to extract simple models of complex software, especially if
we choose the right perspective (e.g., focus on a subset of a component's
functionality) and apply appropriate abstractions.

\paragraph{Protocol State Fuzzing}
Systematic state fuzzing through analysis of protocol state machines obtained
by model learning was first done in 2015, in work that uncovered new
vulnerabilities in TLS implementations~\cite{ruiter2015}.
%
For DTLS, corresponding work was performed by our group~\cite{DTLS@USENIX-20}
in 2020.  We developed a state fuzzing framework,
% DTLS-Fuzzer~\cite{DTLS-Fuzzer@ICST-22},
based on \system{TLS-Attacker}, and applied it to DTLS servers discovering
several previously unknown bugs and vulnerabilities; more in~\cref{sec:prelim}.
%
State fuzzing has also been applied to many other protocols including
TCP~\cite{FJV2016}, SSH~\cite{SSH@SPIN-17},
OpenVPN~\cite{OpenVPN@EuroSPW-2018}, the 802.11 4-Way
Handshake~\cite{stone2018}, and IPsec~\cite{guo2019model}, leading to the
discovery of several security vulnerabilities and non-conformance issues in
their implementations.
%
%% In the works on OpenVPNand the 802.11 4-way Handshake, the detection of
%% state machine bugs is through ocular inspection of models.  In the works on
%% TCP, SSH and IPsec protocol requirements were encoded in linear temporal
%% logic (LTL) and model checked against the learned models.

\paragraph{Differential Testing of Protocol Models}
A related black-box approach for detecting protocol state machine bugs uses
model learning to generate models of several different implementations of a
protocol, and then compares these models to find discrepancies in them.
%
This approach, which has been applied to TCP~\cite{SFADiff},
MQTT~\cite{Tappler@ICST-2017}, QUIC~\cite{Prognosis@SIGCOMM-21} and 4G
LTE~\cite{DIKEUE@CCS-21} protocol implementations, can be regarded as
\emph{differential testing}, but with differences detected in models rather
than in output to individual test inputs.

\paragraph{Model Checking}
Approaches that combine techniques from model learning and model
checking~\cite{MC:handbook} have also been proposed.
%
(They are known as \emph{black box checking}~\cite{BBC} and \emph{adaptive
model checking}~\cite{AdaptiveMC}, an adaptation of model checking to a
black-box setting.)
% , and applied to simple finite-state systems.
%
Most model checkers require a model of the analyzed system, which is typically
provided manually, \eg from some design specification.  The intersection of
this model with representations of the requirements for the system is then
explored for bugs.  This approach is often used for analyzing protocols, \eg
to find attacks on the 4G LTE protocol~\cite{LTEInspector}, on
TCP~\cite{Jero@DSN-15}, or on vehicle protocols~\cite{Hu:usenix21}.
Models can also be obtained using white-box techniques, \eg using symbolic
execution of manually instrumented source code~\cite{Hoque@DSN-17,Themis:ccs21}
or static analysis~\cite{Cao:ccs19}.

\paragraph{Fuzz Testing}
Fuzz testing (or fuzzing) is a dynamic bug discovery technique that tests
systems via a large randomly generated (or mutated) test suite. The goal of
fuzzing is to detect bugs, especially security vulnerabilities, through
testing with unexpected inputs. Black-box fuzzing dates back to 1990 when
Miller \etal~\cite{Fuzz@CACM-90} used it to find crashes in UNIX utilities. In
recent years, due to its ease of use and effectiveness, fuzzing has become a
popular and widely-used testing technique. Coverage-driven grey-box fuzzers
such as \system{AFL}~\cite{AFL} and its derivatives or the OSS-Fuzz service by
Google~\cite{OSS-Fuzz@USENIX-17} have by now found thousands of bugs in
real-world software. Recent surveys on fuzzing
(\eg~\cite{FuzzingSurvey@TSE-21,FuzzingRoadmap@CompSurveys-22}) and
\href{https://wcventure.github.io/FuzzingPaper/}{this webpage} show that
fuzzing is a trending research topic, with a high number of publications each
year in top software engineering and security conferences, which are too many
to properly review here.
%
This success has been possible due to its fully-automated approach, which is
based on unsupervised evolution of fuzz inputs, using simple but at the same
time effective heuristics. In contrast, research on coverage-driven fuzzing
for \emph{stateful} protocols is still at an early stage.
%
Some recent stateful fuzzers~\cite{AFLNET@ICST-20,Snipuzz@CCS-21} infer
protocol states by analyzing the contents of messages (e.g., status codes),
using message parsers that are specifically developed for a single protocol.
% Moreover, it is difficult for these approaches to fuzz many protocols,
% which only embed little or no state information within messages.
Another recent technique, implemented in
\system{StateAFL}~\cite{StateAFL@ESE-22}, tries to achieve effectiveness by
employing \emph{heuristics} (namely, taking snapshots of long-lived memory
areas and applying fuzzy hashing) to recover state information.
%
For achieving success which is similar to the one that coverage-driven fuzzers
for stateless programs often achieve, we believe that more sophisticated
stateful fuzzing techniques need to be developed.


\mycomment{
\section{Significance and Scientific Novelty} \label{sec:significance}
%=====================================================================
\FIX{No text here yet :-(}

\UNREVISED{However, it may be possibe to skip this section if we manage to have short ``Significance'' paragraphs for each of the six tasks in the next section.}
}

\mycomment{
  It is widely recognized that a challenge for software development is
designing and deploying techniques and tools that enhance software
reliability, and allow the automatic detection of software defects, so
called ``bugs.'' Indeed, it is currently the case that identifying and
fixing software bugs accounts for more than 50\% of the development cost of
software, both for Swedish industry, as well as in the rest of the world.
%
Moreover, this cost is rising.
%
One of the reasons for this increase is that, as time passes, a larger
percentage of the programs are becoming not only more complex but also more
concurrent.
%
Concurrent programs are notoriously difficult to get right and test
effectively. Specific interleaving sequences that occur only rarely can
trigger unexpected errors and result in crashes that are surprising even to
expert programmers. Moreover, these bugs are hard to track and cannnot
easily be reproduced; for that reason, they are often referred to as
\textit{heisenbugs}.  Of course, heisenbugs are not a new problem.  The
difference today, compared to \eg two decades ago, is that at that time
concurrent programs would be found mainly in the core of operating systems,
and these were built by specialists.  Nowadays, concurrent programs are
pretty much everywhere, they are often built by relatively inexperienced
programmers, and, to make matters worse, they run on platforms with relaxed
memory.  Besides better awareness of the issues involved, which requires
education, effective techniques and tools that help software developers are
needed.

However, even in software projects where developers are experts, such as in
the Linux kernel, systematic concurrency testing and verification tools are
needed, perhaps today even more than ever.  Nowadays, the Linux kernel is
used in a surprisingly large number of devices: from PCs and servers, to
routers and smart TVs.  For example, in 2015 more than one billion smart
phones used a modified version of the Linux
kernel~\cite{AndroidPhonesKernel}, and in 2016 almost all modern
supercomputers used Linux as well~\cite{SupercomputersKernel}.
%
It should be obvious that the correct and reliable operation of such
software is of great importance for our society.  Unfortunately, the
relatively frequent development cycle of the Linux kernel (there is a new
release every approximately two months), the number of changes that are
involved in each release, and the increasing complexity of the kernel's code
render the verification of its complete code base out-of-reach for current
state-of-the-art techniques; even the partial verification of its components
using explicit-state model checking currently requires too much memory and
time.
%
As we will see in the next section, we have good reasons to believe that
the systematic concurrency testing and stateless model checking techniques
that we have developed can be applied to such code bases with reasonable
requirements in terms of time and memory.
}


\section{Preliminary and Previous Results} \label{sec:prelim}
%============================================================
This proposal builds upon recent work by our group, which we intend to use as
a starting point for some of the tasks in~\cref{sec:description}, but also
significantly extend it.  In this section, we briefly review the main results
of these prior works and indicate how they are relevant for this proposal.

\paragraph{Protocol State Fuzzing}
In work published in USENIX Security 2020~\cite{DTLS@USENIX-20}, we presented
the first comprehensive analysis of DTLS\footnote{The DTLS (Datagram TLS)
protocol is the cryptographic variation of TLS running over UDP. It is one of
the two security protocols in WebRTC, and one of the primary protocols for
securing IoT applications. Our work focused on version 1.2 of DTLS.}
implementations using protocol state fuzzing. To do this, we extended
\system{TLS-Attacker} with support for DTLS, which involved developing
automata learning techniques tailored to the stateless and unreliable nature
of the underlying UDP layer, which is connectionless. We built a framework for
applying protocol state fuzzing on DTLS servers, and used it to learn state
machine models for thirteen DTLS server implementations, including all the
most known ones (OpenSSL, GnuTLS, MbedTLS, JSEE, WolfSSL, and NSS).
%
Analysis of the learned state models revealed four serious security
vulnerabilities, including a full client authentication bypass in the then
latest JSSE (Java Secure Socket Extension) version,\footnote{This security
vulnerability was assigned
\href{https://nvd.nist.gov/vuln/detail/CVE-2020-2655}{CVE-2020-2655}; it
resulted in an Oracle critical patch update and a
\href{https://web-in-security.blogspot.com/2020/01/cve-2020-2655-jsse-client.html}{blog
  post} about it.} as well as several interoperability bugs and
non-conformance issues in most other servers we analyzed.

\paragraph{Tool}
Subsequently, we packaged our work in a model learning and testing tool for
DTLS implementations and also extended it with support for DTLS clients. We
open-source released this tool, called \system{DTLS-Fuzzer}, and described it
in a tool paper~\cite{DTLS-Fuzzer@ICST-22} published in the IEEE Conference on
Software Testing, Verification and Validation (ICST 2022).
%
More information about \system{DTLS-Fuzzer} can be found at its
\href{https://github.com/assist-project/dtls-fuzzer/}{GitHub page}.

\paragraph{Automation}
Although protocol state fuzzing has been used for many protocols, in all prior
works (including our USENIX Security paper), the analysis of the learned state
machine models was performed manually; in most cases using ocular inspection
of the models.  This requires effort and expertise.  Moreover, unsurprisingly,
ocular inspection can miss bugs.  To automate the bug finding capability of
state fuzzing approaches and unleash their full potential, we developed a
\emph{novel} and \emph{general} technique for detecting state machine flaws,
which we published~\cite{AutomataBased@NDSS-23} at the Network and Distributed
System Security (NDSS 2023) Symposium.\footnote{As noted by the PC Chairs in
their opening, our paper was the only paper with authors from Sweden accepted
at NDSS 2023!}
%
Our technique takes as input a catalogue of state machine bugs for the
protocol, each specified as a finite automaton which accepts sequences of
messages that exhibit the bug, and the model of the implementation under
test. It then automatically constructs the set of sequences that (according to
the model) can be performed by the implementation and that (according to the
automaton) expose the bug. These sequences are then transformed to test cases
on the actual implementation to find a witness for the bug. We have applied
our technique on three widely-used implementations of SSH servers (BitVise,
Dropbear, and OpenSSH) and on various versions of nine different DTLS server
and client implementations, and have been able to detect a significant number
of previously unknown bugs and non-conformances in them, including two new
security vulnerabilities.

\paragraph{Fuzzing}
In a parallel line of work, we have been using and evaluating many of the
mutation-based and hybrid fuzzing techniques published in the recent
literature. We have also applied the fuzzers, over a period of more than three
years, on the code base of the Contiki-NG Operating System for IoT devices. In
the process, besides gaining significant experience and expertise in the area
of fuzzing technology, we have been able to detect and correct a significant
number of bugs and security vulnerabilies (which have resulted in a total of
eleven CVEs), in Contiki-NG's code base. We have reported our experiences, as
well as proposed a methodology to evaluate different grey-box fuzzing
techniques, in a paper~\cite{SoManyFuzzers@ASE-22} that appeared in the 2022
ACM/IEEE Conference on Automated Software Engineering (ASE).  Additionally, we
have publicly released a
\href{https://github.com/assist-project/so-many-fuzzers-artifact}{new
  benchmark suite} for fuzzer evaluation.
%
We point out that all eight fuzzers we used in that work are \emph{not}
stateful.


\section{Project Description} \label{sec:description}
%====================================================
This section presents a number of tasks that we plan to pursue in
order to achieve the goals of this project. We organize these tasks along the
three dimensions of work that we briefly outlined at the end of~\cref{sec:aims}.

\subsection{Algorithmic Improvements to Model Learning}
%------------------------------------------------------
%% Model learning algorithms that generate finite-state machine models of
%% protocol implementations have shown effective in generating state machine
%% models of important protocol implementations with even hundreds of states,
%% and have uncovered numerous significant bugs and vulnerabilities
%% [REFERENCES].
%
The main goal of this project is to intend to extend the power of protocol
state fuzzing and stateful grey-box fuzzing techniques for network security
testing.
One key ingredient for achieving this is to \emph{supply the fuzzers with
richer learned models}. One reason is to be able to
capture \emph{data flow}. Protocol specifications
contain numerous requirements on the processing of parameter values in
sequence numbers, identifiers, etc. To check them, corresponding models of
protocol components must describe how different parameter values in sequence
numbers, identifiers, etc.\ are processed and influence the control flow.
Such models often take the form of \emph{extended finite state machines} (EFSMs).

\task{More Effective Algorithms for Learning EFSMs}
%--------------------------------------------------
In recent years, automata learning has been extended to EFSM models that
combine control flow with guards and assignments to data
variables~\cite{CasselHJS16,AJUV15}.
% , and been applied to learn a model of flow control in TCP, discovering subtle bugs in the logic for sequence numbers in major implementations~\cite{FJV2016}.
Unfortunately, the extension to EFSM model carries a significant cost in terms
of the number of tests that must be performed in a black-box setting.
%% For instance, to infer that a branch is taken if an input parameter is greater than $42$ may require some number of tests.
The extension of $L^*$ to EFSM that we have developed~\cite{CasselHJS16}, called $SL^*$, uses a naive technique 
for determining how previous inputs may influence future behaviours, which
requires a number of tests that is \emph{exponential} in the number of data parameters in a
sequence of input packets.
We will reduce this number as follows by applying natural optimizations that
 filter out queries that are not necessary for guaranteeing minimal models.
    % We are currently reimplementing \system{RALib}~\cite{} to incorporate
    % this optimization.
 We can also optimize further    by relaxing the guarantee to generate minimal models.
Then we are free to apply a carefully selected set of input sequences within a given budget, and thereafter find a combination of guards and assignments that agree with the observed test results. This line will use techniques from programming-by-example, which has been successfully used in other contexts~\cite{GulwaniPS17}. Recent work on the Prognosis tool~\cite{Prognosis@SIGCOMM-21} exhibits an \emph{ad hoc} use of this idea.

Another strategy for reducing the test effort needed for model generation is to move away from
the black-box setting and instead apply grey-box or even symbolic white-box techniques. Such techniques allow to
observe directly how data values in previous inputs influence future behaviors. Grey-box and white-box techniques offer
different tradeoffs between the number tests that can be applied within a given time budget, and the level of information that
can be obtained: Greybox-techniques can directly correlate changes in input to change in control flow, whereas white-box
directly reveals which values are stored and which predicates are tested in a trace.
We will investigate the optimal balance in this tradeoff.
The goal here is to develop a principled way to integrate symbolic execution and/or coverage feedback into model learning algorithms.
The resulting technique could replace membership queries (is word $w$ in the language?) by a white- or grey-box
version in which the reply includes the symbolic output (white-box) and/or only coverage information (grey-box) induced by a
sequence of input packets.

\task{Tailoring Model Learning for Security Protocols}
A characterizing feature of security protocols is that they process and generate data using cryptographic algorithms for encryption, decryption, key generation, etc. Specifications of such protocols are carefully analyzed by theorem provers, such as \system{TAMARIN}~\cite{TAMARIN@CAV-13} to verify that they are resilient to various forms of attacks. Such analyses assume precise models of protocol entities that focus on how encrypted data is processed and generated. Up to now, such models are derived from protocol specifications. It would be of great value to be able to obtain such models from actual implementations. We therefore plan to develop a version of the framework for learning EFSMs for security protocols: we
will leverage the commonly used Dolev-Yao attacker model~\cite{DolevYao83} for generating the input. This model describes how attackers can construct inputs from previously exchanged data. Our
model learning framework would use this model to produce test inputs, observe the induced output, and construct a model consistent with these observations.
In order to reverse-engineer output from the SUT, the model learner must have access to cryptographic operations and cryptographic material used by the SUT:
such information is provided in standard test configurations.
The Dolev-Yao model provides a small set of rules for how information in previous interactions can be used by an attacker, implying that the number of test inputs should be tractable for common security protocols, which would make the approach scalable.

% \significance{}

\subsection{Effective Stateful Fuzzing Techniques}
%-------------------------------------------------
We will employ the more efficient model learning algorithms that produce
richer state machines of protocol implementations to enhance the effectiveness
and error detection capabilities of stateful fuzzing techniques.

\task{Protocol State Fuzzing \& Automatic Detection of State Machine Errors}
%---------------------------------------------------------------------------
Unfortunately, employing more advanced model learning algorithms for protocol
state fuzzing purposes is not straightforward.  The main reason is that, as
mentioned in~\cref{sec:SOTA}, to learn realistic SUTs of network protocols,
the learning setup needs to be extended with an appropriate \emph{mapper}.
For complex network protocols, there exists a natural trade-off between the
level of detail in the abstraction that the mapper provides (\eg which and how
many guard predicates it can express, how coarse- or fine-grained the data
domain is, etc.) and the amount of additional information that the richer
state machine models which are learned contain.  To make protocol state
fuzzing based on EFSMs scalable and effective,
% and avoid using unnecessarily approximate state machine models,
we need to develop a general method that strikes the right balance in this
trade-off.
%
It is also likely that this trade-off needs to be inherently
protocol-specific.  I.e., that the method that we will develop may need to
take into account information about the characteristics of the protocol (\eg
how many messages it contains, how many parameters its messages have, how big
the domains of these parameters are, etc.)

For maintaining the full automation in the detection of state machine errors
that we managed to achieve for state machine models with a simple alphabet, of
equal importance is to extend the automata-based algorithm of our recent
NDSS'2023 paper~\cite{AutomataBased@NDSS-23} to work with EFSMs instead of
plain FSMs.  A technical complication is that the technique converts the
learned protocol state machine, which is in the form of a Mealy machine, to a
deterministic finite automaton (DFA) and then uses intersection of DFAs (\ie
intersection of regular languages) as its core ingredient.
% for which efficient algorithms are known.
For richer models, such as EFSMs, whose alphabets also include (arbitrary)
predicates as well as ``plain'' symbols, it is unclear whether suitable such
intersection algorithms exist, let alone efficient ones.  Maintaining
automation may involve translation of the information in EFSMs to some
appropriate language, and use SMT solvers to generate sequences of messages
that expose some state machine bug and validate it in the actual SUT.

% \significance{}

\task{Stateful Grey-box Fuzzing} \FIX{Kostis writes}

% \significance{}

\subsection{Practice}
%--------------------
The third dimension of our work has a more practical flavor, namely to
implement our techniques in tools, apply them in case studies, and developing
an appropriate methodology for their use by protocol implementors. We describe
all these below.

\task{Software Tools \& Case Studies}
%------------------------------------
The main PI has a long history of leading the development of software tools
that are not just prototypes, but instead attract a significant number of
users (cf.\ also~\cref{sec:collaboration}).
%
As mentioned in~\cref{sec:prelim}, we have recently released
\system{DTLS-Fuzzer}, a protocol state fuzzer for DTLS servers and clients.
One direction we will pursue as part of working in this project is to make a
more generic, procotol agnostic protocol state fuzzer, which can be
instantiated with appropriate protocol-specific components by its user.  We
intend to package this tool with components about the main protocols (\eg SSH,
DTLS, QUIQ,~$\ldots$) that we will apply our techniques on, and design a
general interface for its users to write ---and possibly even contribute to
the tool's community--- analogous components for the network protocols of
their interest.

In a similar spirit, we will package the model learning algorithms that we
will develop either into a new stand-alone library or incorporate them
directly into \system{RALib}~\cite{CasselHJS16}.
%
Last but not least, we will implement a stateful coverage-driven grey-box
fuzzer either as a new tool or by extending one of the community efforts to
create a general fuzzer (\eg \system{AFL++}~\cite{AFL++@Woot-20}) that
incorporates techniques published in the literature.

We will evaluate all our techniques and tools on widely-used protocol
implementations, both in order to measure their effectiveness and scalability
(or discover bottlenecks that they may have), but also in order to detect bugs
and security vulnerabilities in them and hopefully make them more robust and
more secure. Of particular interest to use as case studies are implementations
of newly proposed and more lightweight network protocols (such as \eg
EDHOC~\cite{ietf-lake-edhoc-18}), which are put forward as more tailored to
the low energy requirements of IoT systems.

% \significance{}

\task{Methodologies}
%-------------------
Oftentimes, bugs and security vulnerabilities, both in software in general and
in network protocol implementations in particular, manage to survive for a
long time not because techniques and tools to detect them do not exist, but
because the community of their developers is unaware of the proper way to
incorporate them in their development cycle.  We hold that a crycial part of
tool development is to also suggest appropriate methodologies for their most
effective use, and publicize these methodologies, not only on the tools'
homepages, but also in developers' community fora.  We have some prior
experience in this area, and are committed to spend some effort in this task
as part of our engagement in this project.

Another area of our current interest, as also witnessed by our ASE'2022
paper~\cite{SoManyFuzzers@ASE-22}, is to develop and suggest to the research
community appropriate methodologies for the proper evaluation and comparison
of fuzzers.  Although, besides our paper, there is recent work that evaluates
stateless
fuzzers~\cite{EvaluatingFuzz@CCS-18,Magma@POMACS-20,UNIFUZZ@USENIX-21,FIXREVERTER@USENIX-22},
there is very little work on a developing an proper methodology and
appropriate benchmark suites for evaluating the ability of stateful fuzzers to
detect security vulnerabilities. We intend to work on this.

% \significance{}


\mycomment{
\subsection{Algorithmic Improvements to Techniques for Stateless Model Checking}
%----------------------------------------------------------------
Stateless model checking is a technique for systematically exploring
all possible thread interactions that arise in the execution of runs
of a (usually terminating) test of a program and detect any unexpected
test results, program crashes, or assertion violations that occur.
%
In contrast to other (explicit state) model checking techniques, stateless
model checking has very low memory requirements.
%
To combat the combinatorial explosion due to scheduling non-determinism,
\emph{Partial Order Reduction (POR)}
techniques~\cite{Valmari:reduced:state-space,Peled:representatives,Godefroid:thesis,CGMP:partialorder}
are employed to limit the number of explored interleavings,
usually \emph{dynamic} ones such as DPOR~\cite{DPOR@POPL-05}.
%
Till recently, the most effective POR techniques employed the notion
of \emph{persistent sets}~\cite{Godefroid:thesis} (or one of its
variations: \emph{stubborn sets}~\cite{Valmari:reduced:state-space}
and \emph{ample sets}~\cite{CGMP:partialorder}) as their basis. The
use of these sets allows the POR algorithm to explore only a provably
sufficient subset of the enabled processes, and has been the basis of
the area of stateless model checking for the last two decades.
%
In work published at POPL~\cite{DPOR@POPL-14} and was recently
(March~2017) accepted to the Journal of the~ACM, we noticed that the
notion of persistent sets is actually quite weak in that it often
initiates exploration of redundant interleavings and fundamentally
cannot provide optimality guarantees.  We therefore came up with two
fundamental new notions, called \emph{source sets} and \emph{wakeup
  trees}, that can be used as a new, more powerful basis for partial
order reduction. We also developed two new algorithms,
called \emph{source DPOR} and \emph{optimal DPOR}, that significantly
extend the state-of-the-art in this area and outperform all existing
algorithms by a wide margin, often by several orders of magnitude.

Although these algorithms already form a fundamental contribution to
the area, there is still a lot of work to be done.  For starters, the
algorithms only work for programs without data- and control-flow
non-determinism; \ie only when execution of each thread is fully reproducible.
Another open problem is to characterize the space complexity of
optimal DPOR; ideally to design a variation or extension that bounds
the memory required for storing wakeup trees.  Finally, there are also
open questions and extensions of the two DPOR algorithms with more
practical flavour.  We intend to investigate all these as described
below.

\task{Combining Optimal DPOR and Bounding.}
%-----------------------------------------------
Recent progress in testing of concurrent programs includes controlling
the scheduler, in order to force the execution of only some, rather than
all, interleavings. Two of the main techniques that have been investigated
rely on randomized scheduling and on bounding the number of context-switches
(so called \emph{context bounding}), the number of preemptive context
switches (\emph{preemption bounding}), or the number of times a process
can be prohibited from running (\emph{delay bounding}). It has been
claimed that most concurrency bugs can be triggered with relatively
few deviations from a `normal' scheduling~\cite{Randomized@ASPLOS-10},
a claim which has motivated the further development and investigation
of these techniques~\cite{ScheduleBounding@PPoPP-14}.
%
While often very effective in
practice~\cite{BPOR@OOPSLA-13,ScheduleBounding@PPoPP-14}, these heuristics
to limit exhaustive search are rather simple and obtuse.  More importantly,
they interact in very subtle ways with algorithms for dynamic partial order
reduction and optimal DPOR in particular and the \emph{sound} combination
with such algorithms with bounded trace exploration (or a proof that such a
combination is impossible to obtain) is an important open problem for the
area.  For example, even disregarding the optimal DPOR algorithm, which is
very recent, so far none of the existing systematic testing tools has managed
to support fully both DPOR and bounding without exploring redundant traces.
%
Among the first tasks we intend to study is how POR algorithms, and
optimal DPOR in particular, can be combined with bounding techniques
in ways that the resulting algorithm provides guarantees for
both soundness and optimality.

\significance{Currently, this is an open problem in the area of stateless
  model checking with both theoretical and practical significance. Solving
  it in an optimal way seems very challenging.}

\task{Optimality Combined with Fairness.}
%---------------------------------------------
Another challenge in the area is to combine optimal DPOR with a
\emph{fair scheduling} strategy~\cite{FairStatelessMC@PLDI-08}. This
can be considered as an extension of bounding, as it eliminates
interleavings that are impossible to happen on any unmonitored run of
the program due to the design of the actual thread schedulers in
operating systems or process schedulers in virtual machines.  Its
effective combination with DPOR techniques in general and optimal
DPOR in particular is equally challenging.

\significance{This task is of practical nature, but its importance should
  not be underestimated.  Under unfair scheduling strategies, i.e., when
  the scheduler does not give a chance to some processes, stateless model
  checking is not applicable to programs because they create a cyclic state
  space; the only option for verifying these programs is to employ
  considerably heavier explicit-state techniques.}

\task{Symbolic/Concolic Execution Techniques to Guide DPOR.}
%----------------------------------------------------------------
Symbolic execution is a classic technique to enhance the power of testing,
achieve better path coverage and/or guide it towards paths that are more
likely to contain errors. Such symbolic and concolic (concrete+symbolic)
techniques have been significantly refined in recent years; see a survey
article in \textit{CACM}~\cite{SymbolicTesting@CACM-13}.
% and a recent (Nov.~2015) keynote~\cite{Concolic@WADA-15}.
Despite its advances for sequential programs, the
technique is still not well developed for concurrent programs: better
symbolic approaches are needed to efficiently avoid redundant
exploration of similar executions. For this, inspiration could be
taken from recently published
works~\cite{MCR@PLDI-15,SATCheck@OOPSLA-15,MCR-TSO@OOPSLA-16} that employ
SAT solvers to obtain considerable, often maximal, reduction in the number
of explored interleavings.
%
Another approach is to combine techniques for symbolic execution with
techniques to cover different schedulings with minimal effort (\eg
the algorithms for targeted selection of interleavings described in
the previous paragraph).
%
In any case, techniques from symbolic/concolic execution are clearly needed
for programs that depend on external inputs or contain forms of data
non-determinism.

\significance{An effective technique for handling data non-determinism is
  needed not only for many practical applications, but also in order to be
  able to compare in a better way the pros and cons of stateless vs.\/
  explicit-state model checking techniques and tools.}
}

\mycomment{
\subsection{Formal Models and Connection with Modern Programming Languages}
%--------------------------------------------------------------------------

\task{A Foundation for Concolic Execution of Programs in Higher-Order Languages.}
%---------------------------------------------------------------
Concolic execution has so far mainly been explored in the context of
low-level languages such as C~\cite{CUTE@FSE-05} or LLVM
assembly~\cite{KLEE@OSDI-08}.  Recently, we have extended concolic
execution to functional languages, and described in detail how to take
pattern matching into account~\cite{CutEr@PPDP-15}.  However, more and more
modern programming languages also come with higher-order constructs (\eg
lambdas, closures, etc.).  Whether it is possible for a concolic
engine to handle such constructs and, if so, what is the best way to do
this is not clear.  Equally challenging is to map higher-order functions to
the language of modern SAT solvers such as Z3~\cite{Z3} or
CVC4~\cite{CVC4@CAV-11}.
%
We intent to investigate these issues by developing a suitable formalism
for concolic execution of higher-order programs that can also be proven
correct.  We will initially target higher-order functional languages
(Erlang in particular), but we will subsequently extend the formalism to
also include more mainstream languages with support for higher-order
constructs (\eg Scala). Needless to say, this effort will complement the
previous task that aims to combine DPOR with techniques from concolic
execution, since both these languages come with built-in support for
concurrency.

\significance{This will be the first time that concolic execution of
  higher-order programs will be formalized.  Such a formalism could become
  a ``standard'' reference in the area and may result in widespread
  adoption of this important testing technique by programmers of these
  languages as well.}


\task{Model Checking C11/C++11 Programs.} \label{task:rcmc}
%----------------------------------------------------------
We plan to build a model checker for C/C++ programs that use the
synchronisation primitives (atomic accesses and fences) introduced by the
2011 versions of the C and C++ standards and adhere to the concurrency
model of these standards. These synchronisation primitives allow writing
efficient architecture-independent concurrent code and are increasingly
being adopted as they are natively supported by the mainstream C/C++
compilers. Currently, however, there exist only a few ad hoc techniques for
testing C/C++ programs with these primitives; these techniques either
provide no formal guarantees of coverage nor of optimality. We intend to
leverage the optimal DPOR work to provide both these guarantees: exploring
all possible executions of a program (up to some loop unrolling depth)
without exploring the same execution twice. The main challenge in achieving
this is that the C/C++ concurrency model is defined in an axiomatic style
(whereas SC and TSO also have operational definitions) and so the
exploration algorithm will directly have to consider partially-ordered
executions rather than execution traces.

Besides the obvious benefit of enabling verification of standard-adhering
concurrent C/C++ programs, we expect that this work will also assist the
development of the C/C++ standards per se. The C/C++ standards are
currently undergoing revision, as a number of subtle problems have been
identified in their concurrency models, and some fixes have been
proposed. There are long discussions about these fixes, largely because
their impact on existing software is unclear. We believe that an automated
verification tool capable of testing existing code with respect to the
C/C++ model and slight modifications thereof will prove indispensable in
the process, not only speeding up the discussions of the standards
committee, but also avoiding the introduction of further subtle errors in
the standard.

\significance{Techniques and tools that are able to handle the subtleties
  of modern versions of C and C++ concurrency are currently lacking. This
  is a problem, not only for program developers, but also for programming
  language researchers and committees that need to standardize and further
  extend these languages in an ever-changing landscape of architectures
  with relaxed memory.}
}

\mycomment{
\task{Methodology for Test Specifications.}
%-----------------------------------------------
The question of how much and what to test in order to maximize
coverage is still to a large degree unanswered.  We will try to
address this question.
%
One approach could be to develop and systematically test concurrent
programs using an enhanced version of \system{Concuerror} (which
handles scheduling non-determinism) with a technique that employs a
concolic tool, such as \system{CutEr}~\cite{CutEr@PPDP-15}, to handle
data (input) non-determinism.
%
On a related topic, an interesting open question is the following:
Given a test case that shows the absence or presence of concurrency
errors, how can one minimize this test (\eg decrease the number of
processes involved in it) and still be certain that the test continues
to test the absence or presence of the same set of concurrency errors?
Though the general case of the problem is obviously undecidable, one
could possibly go far by mapping the actions that a systematic testing
tool has to do in order to avoid exploration of interleavings during
execution back to the source code of the test.

\significance{These tasks are of practical nature, but they are
  important for achieving impact and make these testing techniques
  more easily adopted by programmers out there.}
}


\mycomment{
\subsection{Tools and Case Studies}
%----------------------------------------
In order to demonstrate the applicability of our techniques in the
``real-world'', we intend to also work on issues that have more of an
engineering and practical flavor.

\task{Tool Development and Support.}
%---------------------------------------------
Our group has a long tradition of creating open source software tools that
are not just prototypes, but instead attract a significant number of users.
In the area of this proposal, we have recently released the first version
of a concolic testing tool for Erlang, called \system{CutEr}
(\url{github.com/aggelgian/cuter}), that has already managed to build a
sizable user community.\footnote{In March 2017, CutEr had 165
  ``stargazers'' on GitHub and, most likely, more users than this number.}
%
Also, the DPOR algorithms that we have designed form the basis of
\system{Concuerror} (\url{www.concuerror.com}), a stateless model checking
tool for Erlang, and \system{Nidhugg} (\url{github.com/nidhugg/nidhugg}), a
stateless model checking tool for C and C++ programs using the pthread
library.

Throughout the duration of this project, we will make an effort to
incorporate our techniques into these tools, and will be releasing them to
the user community of their target language(s).  In addition, we will try
to engage their users, not only to provide us with feedback, but also in
order to give us access to code bases where we can conduct case studies,
evaluate the effectiveness of our techniques, and get inspiration for new
ones that may be needed.
%
% We believe we are in a unique position to do so.
%
Additionally, we will complement the user manuals of the tools with
tutorials, and we will try to present them both to scientific conferences
of the area and to suitable developer conferences.

\task{Effective Visualization of Concurrency Errors.}
%----------------------------------------------------
This is a relatively small task, but our experiences tell us that it is an
important one.  Once a stateless model checking tool has found a
concurrency error, understanding the chain of events that have caused it
can be extremely difficult, even for the developers that originally wrote
the code. The output of the tool is often just a textual log of the events
that have lead up to the error.  Even after such logs have been manually
minimized, the real cause for the error can still be difficult to see;
especially if the error involves some effects from relaxed memory.  We plan
to design and implement techniques for visualizing concurrency errors in
better ways, most likely graphical ones, and incorporate them in our tools.
The goal is to make reports of concurrency errors as easy for programmers
to understand as compiler errors are in modern compilers.

\task{Case Studies.}
%-------------------
As mentioned in the beginning, one of the goals of this proposal is to
demonstrate the applicability and benefits that the techniques we will
develop bring to code bases of significant size.  One code base we intend
to use for cases studies is the Linux kernel.  Besides being open source
and widely used, it is also of significant size and quite complex from the
perspective of concurrency primitives it uses.  Its code base is thus quite
challenging for most techniques that detect concurrency bugs or verify
their absence.  However, for this exact reason, it is also suitable for
serving as an ``acid test'', for attracting interest from the community and
achieving high impact.

Another set of case studies we intend to perform is testing and
verification of (implementations of) protocols for sensor networks and for
scalable distributed programming.  A concurrent and distributed language
such as Erlang, is ideal for \emph{modeling} sensor network protocols or
protocols for scalable distribution.  Given such a model, a stateless model
checking tool like \system{Concuerror} can then be used to detect bugs in
the protocol or verify that the protocol behaves correctly in all possible
executions (interleavings of processes in the model).  We are currently
working on verifying the correctness of a distributed algorithm (a shared
log design of flush clusters) using such an approach.
}


\section{National and International Collaboration} \label{sec:collaboration}
%===========================================================================
Within Sweden, besides a long collaboration with the participating researcher
(Prof.~Bengt Jonsson), the group of the main applicant collaborates with
research groups within the IT department of Uppsala University, most notably
with the verification group (Profs.~Parosh Abdulla and Mohamed Fauzi Atig),
the computer architecture group (Prof.~Stefanos Kaxiras) on implementing the
ArgoDSM system, and the sensor network group (Prof.~Thiemo Voigt) on testing
sensor networks~\cite{PBT@SECON-15}.
%
Since 1999, he has maintained a close collaboration with the
\system{Erlang/OTP} development team at Ericsson, on issues related to the
implementation and evolution of the Erlang language, its native code compiler
(\system{HiPE}), the \system{Dialyzer} static analysis tool, and
\system{TypEr}. All these, which have been built by the group of the PI, have
been part of the Erlang/OTP (Open Telecom Platform) distribution, and are
actively used by the Erlang and Elixir programming communities.

In the context of \href{https://assist-project.github.io}{SSF aSSIsT project},
he has collaborated with researchers at RISE in using fuzzing to detect and
fix security vulnerabilities in the
\href{https://www.contiki-ng.org/}{Contiki-NG} OS for Next Generation IoT
devices.

Internationally, the PI has collaborated with the group of Viktor Vafeiadis at
the Max Plank Institute for Software Systems (Germany) on algorithms and tools
for model checking C/C++11 programs, with
% Paul McKenney, a Distinguished Engineer
engineers at the IBM Linux Technology Center (USA) on the verification of
RCU, a key component of the Linux kernel, and
% Scott Lystig Fritchie
at VMware Research (USA) on verification of protocols for repairing
distributed clusters after a failure.
%
He has also collaborated with members of the
\href{http://release-project.eu/}{RELEASE EU project} (ended 2015) at the
Universities of Glasgow and Kent (both in the U.K.) on developing a high-level
paradigm for large-scale server software, and with groups at the Barcelona
Supercomputing Center (Spain) and Fraunhofer (Germany) as part of the
\href{https://epeec-project.eu/}{EPEEC EU project} (ended 2022) on a
productive programming environment for Exascale Computing.

International collaborations directly relevant to this proposal are with:
\begin{inparaenum}[(1)]
\item Researchers of the Network and Data Security group at Ruhr-Universit\"at
  Bochum, and with Prof. Juraj Somorovsky at the University of Paderborn (both
  in Germany), on techniques for protocol state fuzzing and applying them on
  (D)TLS implementations;
\item Dr.~Joeri de Ruiter, a researcher and specialist for security and
  privacy at SIDN Labs (Netherlands), also on protocol state fuzzing
  techniques;
\item Dr.~Thanassis Avgerinos, a former student of the main PI, who has been a
  team member of the Mayhem Cyber Reasoning system and is now Founder and Vice
  President of Engineering of ForAllSecure Inc.\ (USA); and
\item Prof.~Falk Howar, Chair of Software Engineering at the Dortmund
  University of Technology (Germany) working on automata learning.
\end{inparaenum}

\section{Other Applications, Grants, and Activities}
%===================================================
We request funds to cover the yearly salary of a Post Doctoral researcher.
We expect that we will employ a total of two PostDocs during the duration of
this project, quite likely overlapping in time during 2025--2026.
%
In addition, we request some funding (15\%) for the main applicant who is
committed to be involved 20\% of his time in this project, and some moderate
funds for open access publication costs, conference participation, and
research visits.
%
This application is \emph{not} submitted to any other funding agency.

%\vspace*{-.5em}
%\paragraph{Other Grants.}
\noindent
The only other funding of the main PI comes from its involvement in:
\vspace*{-.5em}
\begin{quoting}
  \textbf{aSSIsT: Secure Software for the Internet of Things}.
  A Project on Cybersecurity from the Swedish Foundation for Strategic
  Research (SSF), July 2018--2022. \url{https://assist-project.github.io}
\end{quoting}
\vspace*{-.5em}
%
This is a joint grant between Uppsala University and RISE (Research Institutes
of Sweden) involving four PIs (two at UU and two at RISE).  The aSSIsT project
was supposed to have already ended, but has been allowed to use its remaining
funds also in 2024. (No further extension is allowed.) At UU, these remaining
funds currently support two PostDocs and two Ph.D.\ students working on
various aspects of IoT security and correctness.  Both Ph.D.\ students are
expected to graduate during 2024.

Uppsala University has Cybersecurity as one of its prioritized areas.  The
Division of Computing Science (Datalogi), where the main PI belongs, is
currently recruiting a new Associate Professor (``BUL'') with specialization
in Cybersecurity. (The position closed in March 2023; 31 candidates have
applied.)  The IT Department is also partly supporting activities of
Ph.D.\ students working in the area of Cybersecurity, and runs a regular
seminar series in the area.  An upcoming related activity is SWITS (Swedish IT
Security Network for PhD students), organized in 29--30th of May 2023 in
Uppsala at the premises of the IT Department.

Last but not least, both PIs of this project regularly supervise MSc theses in
the area of cybersecurity and software correctness, and incorporate aspects of
their research and the tools that their groups have produced in undergraduate
and graduate courses that they are involved.  Occasionally, we are fortunate
enough to excite students to the point that they become contributors to the
tools' development. In any case, we strongly believe that an important aspect
of being part of a research University is to expose students to new techniques
and tools, and raise their awareness of issues that are related to the
importance of network and software security.


%======================================================================
\bibliographystyle{abbrv}
\bibliography{trimmed}
%======================================================================

\end{document}
